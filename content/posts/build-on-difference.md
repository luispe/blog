---
categories:
- idp
date: "2025-11-01T00:00:00-03:00"
description: The how and why behind our IDP â€” built from our differences
tags:
- platform-engineering
- idp
title: Building from Differences
toc: true
---

In this post, Iâ€™ll try to share our experience building Akuaâ€™s IDP â€” the challenges we faced, the current state, and what lies ahead.

With [Ger](https://www.linkedin.com/in/geryepes/) â€” at this point I can proudly call him a friend â€” we joined Akua from the very beginning with the goal of building the internal developer platform. We both came from different industries and backgrounds, but shared a deep common vision of what Akua needed.

I left the link to his LinkedIn, but to give a **very short (and disrespectfully brief)** summary: Ger comes with experience from the lowest layers of infrastructure all the way up to building the team and platform at *Sate* (for friends), or [Satellogic](https://www.linkedin.com/company/satellogic/) if weâ€™re being formal.

In my case, I started my career as a product developer and, during my time at Viacom (TelefÃ©), I began to feel a strong curiosity for infrastructure. At Pomelo, I had the chance to work full-time helping to build the IDP, and at Akua, together with my â€œcompa,â€ we designed and executed it from scratch â€” something Iâ€™m incredibly proud of.

## Early Days and First Steps

We started literally with a blank page. There was absolutely nothing in Akua â€” just an empty AWS account â€” but that was the least of our concerns.
Hereâ€™s a list of our â€œmantras,â€ which we still uphold to this day:

- Platform operation and maintenance should tend toward zero.
- Everything we build must be testable and runnable locally â€” without being a nightmare to do so.
- If it works in development, it must work in production (or any other lifecycle stage).
- Everything should be self-discoverable â€” no hardcoding.
- Simplicity above all.
- Break the inertia of past experiences.

### Laying the Foundations

We partnered with Binbash to execute our PCI-compliant network design, while Ger and I evaluated which tool would best fit our infrastructure management needs for the IDP.
To be fully transparent, we evaluated three tools â€” discarded one immediately, and ran quick POCs with the other two before deciding.

The three tools we analyzed:

- **Terraform CDK** â€” We discarded it from day one. Why? Terraformâ€™s direction was becoming less open source, and (at least at that time) Terraform CDK lacked proper documentation â€” something that kills us both given our working style.

- **Crossplane** â€” Tempting at first, but as soon as you need to add logic to a product your platform provides, youâ€™re in trouble. Letâ€™s be honest â€” YAML is too fragile to handle platform product logic.

- **Pulumi** â€” You probably guessed it: we chose Pulumi. It gave us flexibility to implement any logic we wanted, abstracted away the state management complexity, had stellar documentation, and strong community adoption. That last point mattered a lot â€” when we onboard new teammates, they wonâ€™t be facing an obscure stack.
  And not least â€” Pulumi lets you implement **Dynamic Providers**, meaning if a resource isnâ€™t supported, you can still manage it via Pulumi interfaces. In our case, we had to do that twice â€” once with Typesense and another with â€œdeployment markersâ€ in New Relic.

I should clarify â€” neither of us had used Pulumi in production before. We had tested and explored it, but this was our first time using it in a real-world, mission-critical context.

Once we chose our backend tool, we moved to the next layer of our IDP: the presentation layer.

As the title suggests, hereâ€™s where our **differences** came into play.
Ger, coming from a deeply technical background and teams that value full control, proposed exposing platform products directly â€” meaning developers would use Pulumi themselves.
My take was different â€” I believed Akuaâ€™s developers wouldnâ€™t feel comfortable having to use a platform tool directly. We needed a **presentation layer (aka portal)** where they could design, deploy, and manage projects.

For this layer, we analyzed two tools: **Backstage** and **Port.io**.
There are more, of course, but as always, we focused on those with strong adoption across the industry â€” so anyone joining the team wouldnâ€™t face something too niche.

We ended up choosing **Port.io**. Why? Backstage wouldâ€™ve required us to build custom frontend components â€” something outside our expertise and timeline.
Port.io, on the other hand, wasnâ€™t a fallback choice â€” itâ€™s like the *Notion for platforms*. Its Blueprint system lets you design your platform around your needs, not the other way around.
The UI is elegant, it supports SSO, has RBAC (still room for improvement), and includes Scorecards, Self-Service Actions, Automations, and click-and-build dashboards.
All in all, it became the perfect tool for our platformâ€™s presentation layer.

### Technology Stack

We built a custom Helm chart for our applications. Over time, it evolved and now supports both microservices and monorepos â€” meaning multiple services can be deployed from a single GitLab project without having to manage â€œnâ€ repositories.

Our Helm chart is centralized, versioned (using semver), and evolves like any other piece of software. Product developers can choose whichever version they need.
The idea was to **centralize evolution** while minimizing cognitive load for developers.
Each project defines its own `values.yaml`, which are used at deployment time â€” thatâ€™s it.

For CI/CD, we use **GitLab**. Gerâ€™s experience here was key â€” we use private runners that sync application resources without distributing AWS credentials across GitLab, avoiding major security risks.

Speaking of GitLab, we know from experience that pipeline updates are a pain when every project has its own.
So we built **centralized pipelines**, and projects simply `include` them.
Need something new? Add it to the central pipeline, and everyone gets it automatically â€” semver, of course, to evolve without breaking things.

As for orchestration â€” yes, we use **Kubernetes**. But with a twist: **EKS on Fargate**.
That was Gerâ€™s proposal, and thankfully we convinced everyone to go that route.

Why Fargate?
- Almost zero operational overhead.
- Upgrading Kubernetes versions is trivial.
- PCI compliant.
- And overall, much simpler.

Of course, there are trade-offs â€” for example, you canâ€™t deploy DaemonSets â€” but weâ€™re fine with that. Our mantra is **keep it simple**, and our EKS clusters only run:
- Kong
- Metrics Server
- External DNS

And yes, installing anything else needs a **very strong justification**.

#### Wait, what about secrets?

We built a workflow with **Parameter Store** that lets us deploy and inject secrets into Kubernetes applications.
If a secret isnâ€™t managed by our infra-lib, the security team can still add it to Parameter Store, and our infra-lib takes care of the rest.

#### Security â€” Auth & Authz

From day one, weâ€™ve used **IRSA (IAM Roles for Service Accounts)** and **tag-based permissions** (more on that later).
If an AWS resource doesnâ€™t support tagging (like S3), our infra-lib auto-discovers it and applies the right permissions.

Someone once commented on a previous post that â€œa platform without FinOps canâ€™t be called a platform.â€
So, letâ€™s clarify â€” all our infrastructure resources are centrally tagged by our infra-lib.
Thatâ€™s what will make it easy to implement cost tracking and billing later.
And a quick side note â€” our infra costs turned out to be *significantly lower* than we had projected, thanks to proper environment configurations handled by our infra-lib.

Continuing with security â€” database access is controlled via **Security Group rules**, and communication between applications is blocked at the network level (even within the same namespace).
This allows us to define and enforce access rules between services through **Kong**, easily configurable from our Port.io portal.

## Conclusions

Thereâ€™s still so much to share â€” how we implemented Scorecards, Day-2 actions, single-tenant deployments, public route publishingâ€¦
But Iâ€™ll save that for another post :)

For now, Iâ€™d like to close with a reflection.

As I mentioned, we didnâ€™t always agree â€” but we learned to build our internal developer platform *from our differences*, while always respecting each otherâ€™s perspective.
We created a workflow that led us to design and deliver solid, long-term solutions.
Were all the ideas Gerâ€™s? Mine? Honestly, it doesnâ€™t matter.
We built a shared process where we analyze, design, and evaluate multiple approaches before implementing â€” real teamwork, where what truly matters is the platform and, above all, the people behind it.

Ger, my friend â€” thank you deeply for your generosity and for teaching me so much.
I hope I left some marks of my own too.
Itâ€™s been an honor and a joy to build this platform â€” and even more so, the friendship that came with it. That oneâ€™s for life â¤ï¸

See you next time ğŸ‘‹ğŸ½
