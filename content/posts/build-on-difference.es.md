---
categories:
- idp
date: "2025-11-01T00:00:00-03:00"
description: El c칩mo y el porqu칠 de nuestro IDP desde las diferencias
tags:
- platform-engineering
- idp
title: Construir desde las diferencias
toc: true
---


En esta publicaci칩n voy intentar contar nuestra experiencia
construyendo el IDP en Akua, que retos sorteamos, estado actual
y lo que queda para el futuro del IDP.

Con [Ger](https://www.linkedin.com/in/geryepes/) a esta altura me
animo a decir que un amigo, nos unimos desde los inicios de Akua
con el objetivo de constuir la plataforma de desarrollo interna,
ambos de experiencias e industrias diferentes pero con una
misma visi칩n profunda sobre lo que necesitaba Akua.

Deje enlace de su linkedin, pero a muy breve --e irrespetuoso resumen--
Ger viene con una experiencia desde el mas bajo nivel en infraestructura
hasta llegar a constuir el equipo y la plataforma en Sate (para los amigos),
[Satellogic](https://www.linkedin.com/company/satellogic/) si nos ponemos
mas serios.

En mi caso arranque mi carrera como desarrollador de producto y ya desde
mi paso por Viacom (Telef칠) comenc칠 a sentir una curiosidad muy fuerte
por la infraestructura, en Pomelo pude trabajar a tiempo completo ayudando
a construir el IDP y en Akua ya junto a "mi compa" lo dise침amos y ejecutamos
desde cero con todo el orgullo que eso me provoca.

## Primeros dias y primeros pasos

Comenzamos con una hoja en blanco, literal, no hab칤a absolutamente nada
en Akua, una cuenta de AWS pelada pero eso era lo que menos nos preocupaba;
dejo un punteo de "nuestros mantras" que hasta el dia de hoy seguimos
manteniendo y velando porque eso siga sucediendo:

- La operaci칩n y mantenimiento de nuestra plataforma debe tender a cero.
- Lo que desarrollemos se tiene que poder probar y testear local sin que sea
un chino poder hacerlo.
- Si funciona en desarrollo tiene que funcionar en producci칩n o en cualquier
ciclo de vida por el que pase el software.
- Todo debe autodescubirse, no "harcodiemos" nada.
- La simpleza ante todo.
- Rompamos con la inercia de experiencias previas.

### Armando las bases

Nos aliamos con Binbash para que ejecute nuestro dise침o de redes
compliance con PCI, mientras Ger y yo hac칤amos una validaci칩n
de que herramienta se iba a adaptar mejor para la gesti칩n de la
infraestructura de nuestro IDP, para ser absolutamente transparentes
evaluamos tres, desde el inicio descartamos una y con las dos restantes
hicimos unas pruebas rapidas de concepto para determinar con cual nos
칤bamos a quedar.

Las tres herramientas analizadas:
- **Terraform CDK**, de las tres analizadas la descartamos desde el d칤a cero,
porqu칠?, terraform venia con una direcci칩n no open source y sumado a eso
vimos (al menos en ese momento) que al proyecto de Terraform CDK
le faltaba una enorme documentaci칩n, algo que a Ger y a m칤 nos
mata por nuetra forma de trabajo.

- **Crossplane**, puede ser tentador utilizar esta herramienta, pero en la
primera de cambio que tengas que agregar algo de l칩gica en un producto que
provea tu plataforma estas un poco sonado, y seamos honestos, YAML es demasiado
fr치gil para delegarle la responsabilidad de gestionar los productos que
ofrezca nuestro IDP.

- **Pulumi**, ya te habr치s dado cuenta, elegimos Pulumi, nos daba flexibilidad
para poder desarrollar cualquier l칩gica que queramos, nos abstra칤a de la
complejidad de manejar el estado de los recursos, su documentaci칩n es un lujazo
y tiene una fuerte adopci칩n en la comunidad; lo que nos proyectaba que en caso
futuro si sumamos a mas compas al equipo no se van a encontrar con algo totalmente
desconocido; por 칰ltimo y no menor te ofrece la posibiliad de implementar
dynamic resources osea si hay algun recurso que no este disponible en Pulumi
uno puede implementar la interfaz de Pulumi y gestionar recursos no soportados
por el IaC, en nuestro caso si no recuerdo mal tuvimos que hacerlo en
dos oportunidades con Typensense y "marcas de deployment" en NewRelic.

Quiero hacer una aclaraci칩n, ni Ger ni yo hab칤amos usado de forma productiva a
Pulumi, conoc칤amos y hab칤amos testeado la herramienta, pero no hab칤amos tenido
oportunidad de usarla de forma productiva.

Una vez que elegimos nuestra herramienta de backend pasamos a la siguiente
fase de definici칩n de nuetro IDP, la capa de presentaci칩n.

Como dice el t칤tulo ac치 pudimos debatir desde las diferencias que 칤bamos a
ofrecer, Ger desde su experiencia con equipos extremandamente t칠cnicos y que
tienen una necesidad de control de absolutamente todo propon칤a ofrecer las
abstracciones de los productos de plataforma y que nuestros desarrolladores
usen en sus proyectos dicha abstracci칩n --aka iban a tener que saber manejar pulumi--;
en contraparte mi l칤nea de pensamiento y propuesta era que los
desarrolladores de Akua no 칤ban a sentirse c칩modos teniendo que
saber manejar una herramienta de plataforma, deb칤amos ofrecer una capa
de presentacion --aka portal-- y que desde ah칤 puedan dise침ar la
arquitectura del proyecto y posterior despliegue y gesti칩n.

Ac치 al igual que nuestra herramienta de backend hoy en d칤a aparecen
varias posibilidades, analizamos dos, Backstage y Port.io,
hay mas claro, pero como siempre buscamos las de mayor adopci칩n en la
industria para que el d칤a de ma침ana quien ingrese al equipo no se encuentre
con algo que no se usa en ning칰n lado.

En este caso nos decidimos por Port.io, porqu칠? Backstage nos obligaba a
tener que desarrollar cosas arriba y en el momento en el que
estabamos no quer칤amos tocar nada de frontend, no es nuestro fuerte y nos
oblibaga a tener un desvio de tiempos importante que no eran una opci칩n.

En el caso de Port.io no fue por descarte, me gusta hablar de Port como
el Notion de las plataformas, su sistema de Bluperints te permite dise침ar
tu plataforma a lo que necesitas y no te obliga a adaptarte a sus decisiones,
la UI es exquisita, brinda acceso por SSO, tiene sistema de RBAC (aun bastante mejorable)
y sumado a Scorecards, Self Service actions, Automations y generaci칩n de dashboard
con un par de clicks se convirti칩 en nuestra herramienta perfecta para la capa
de presentaci칩n de nuestra Plataforma que estabamos buscando.

### Stack tecnol칩gico

Desarrollamos un Helm chart a medida para nuestras aplicaciones, con el tiempo
evolucion칩 y hoy soporta poder desplegar tanto un microservicio o un monorepo con esto
칰ltimo nos queremos referir a que con un mismo proyecto se puedan desplegar m칰ltiples
servicios en kubernetes sin necesidad de tener "n" proyectos en Gitlab.

Nuestro helm chart (tambi칠n pensado desde el d칤a cero) esta centralizado y
evoluciona como cualquier software con semver, y nuestros desarrolladores
de producto tienen la libertad de usar x o y version si necesitan
de dicha funcionalidad.

Ac치 una vez mas nuestro enfoque fue, centralicemos la herramienta y la evoluci칩n,
intentando no transferir carga cognitiva a nuestros clientes; asi que en este punto
nuestro helm-chart evoluciona y cada proyecto definie sus values.yaml que en tiempo
de despliegue se utilizan para el despliegue de la(s) aplicaci칩n(es).

Para nuestra gesti칩n de ciclo de vida de las aplicaciones usamos gitlab, Ger en este caso
tiene enorme experiencia y nos sirvio un mont칩n para tener runners privados que ejecutan la
sincronizaci칩n de los recursos de la aplicaci칩n sin necesidad de tener credenciales de
acceso a AWS distribuidas en Gitlab, con todo el peligro que ello conlleva.

Hablando de Gitlab, tambi칠n por experiencias previas, sabemos que realizar cambios
en los pipelines y que todos los proyectos los adopten es un desaf칤o, por eso fuimos
por un esquema de pipelines centralizados y que los proyectos solo hagn el "include"
y no se preocupen de nada mas, en caso de necesitar algo se agrega al pipeline centralizado
y ya todos los proyectos lo tienen disponible, como siempre semver para poder evolucionar
sin comprometer el desarrollo de nuestros clientes.

Creo que ya lo coment칠 pero para la orquestaci칩n y carga de trabajo de nuestras
aplicaciones usamos kubernetes, en este caso con una particularidad, lo usamos
con fargate, propuesta del Ger y por suerte adoptada y pudimos convencer a las
personas necesarias para ir por este camino.

Que ventaja nos da usar fargate? operaci칩n de kubernetes casi nula, actualizar la
version de kubernetes es trivial, es PCI compliance, entre otros befenicios.

Tiene sus contras como todo, no se pueden instalar herramienteas en kubernetes
que necesitan desplegar un daemonset, igual mucho no nos preocupa eso porque
(volviendo a uno de nuestros mantras de mantener todo lo mas simple posible),
nuestro cluster de EKS tiene muy muy pocas herramientas o componentes instalados:
- Kong
- Metrics server
- External DNS

Y si, tienen que haber una justificaci칩n y ganancia muy alta para instalar
mas herramientas en nuestros clusters.

#### Un momento y los secretos?

Desarrollamos un flujo de trabajo con parameter store que nos permite desplegear
y disponibilizar los secretos a las aplicaciones en kuberentes con la flexibilidad
de que si algun secreto no es manejado por nuestra infra-lib el equipo de seguridad
lo pueda agregar a parameter store y nuestra infra-lib lo deja disponible en la app.


#### Seguridad auth y authz?

Desde el minuto cero para las aplicaciones usamos IRSA (Iam Role for Service Account)
y desplegamos permisos por tag (en breve comento nuestra estrategia de tagging),
y en caso de que el recurso de AWS no lo soporte (s3), nuestra infra-lib tiene
la habilidad de autodescubrir el recurso y asignar los permisos necesarios.

Acabo de hablar de tag y recuerdo que en alguna publicaci칩n que hicimos
alguien nos critic칩 que una plataforma sin finops no puede considerearse como tal,
aprovecho a contestarle que desde el minuto cero todos nuestros recursos de infraestructura
cuentan con tags centralizados manejados por nuestra infra-lib, esto nos va a permitir a
futuro cuando desarrollemos e implementemos el concepto de billing para nuestro IDP
sea todo mas facil al tener nuestro recursos correctamente "taggeados" y le dejo una
breve PD: el gasto de nuestra infraestructura es bastante mas bajo del que hab칤amos
predicho gracias a las correctas configuraciones por entorno que hace nuestra infra-lib
en los recursos de infraestructura.

Siguiendo con la tem치tica de seguridad, el acceso a los recursos como base de datos
es mediante reglas en los security groups de la app y la DB, asi mismo el acceso
entre aplicaciones est치 bloqueado a nivel del red aunque esten el mismo namespace,
esto nos permite establecer reglas de acceso entre servicios que son manejadas por
Kong y se configuran de forma f치cil desde nuestro portal Port.io.

## Conclusiones

Resta a칰n comentar un monton de cosas mas: como implementamos scorecards,
acciones del dia 2, despliegue de single tenants, publicaci칩n de rutas p칰blicas;
entre otros, pero lo guardo para otra publicaci칩n :).

Pero para ir finalizando quiero reflexionar sobre algunos asuntos.

Para comenzar como coment칠 no en todos los casos estuvimos de
acuerdo con las propuestas tra칤das por uno u otro, pero
supimos construir una plataforma de desarrollo interna desde
nuestras diferencias pero respetando siempre las opiniones
de cada uno; constuirmos un flujo de trabajo que nos llev칩 a
desarrollar soluciones s칩lidas con proyecci칩n para varios a침os y
esto siendo sinceros todas fueron ideas de Ger? o fueron ideas m칤as?
poco importa sinceramente, como les contaba constuimos un flujo de
laburo donde dise침amos y evaluamos multiples aleternativas y casu칤sticas
para luego implementar; al final del camino es realmente un trabajo
en equipo donde lo que prevalece es nuestra plataforma y por sobre
todo las personas que lo integran.

Ger querido, agradezco enormemente tu gratitud y apertura para
ense침arme una enorme cantidad de cosas, espero haberte dejado
marcas de las m칤as; es un orgullo y enorme placer haber construido
juntos esta plataforma y lo mas importante la amistad que
construimos y eso es para toda la vida 仇벒잺!

Hasta la pr칩xima 游녦游낗
