{"/blog/about/":{"data":{"":"In the blog you will find reflections and learnings about my life in general.\nI do not pretend to be an example of anything or anyone, the opinions I leave here are mine, I hope not to offend anyone with them.\nMy name is Luis Pedro -luispe- I am 38 years old, I was born and raised in Gonnet, a town in La Plata, BsAs, Argentina. From my childhood I keep a ton of friends with whom I have shared 35 years, now that I am writing this is crazy. They are my guide and my grounding cable, if I am what I am it is in part thanks to them and my family ‚ù§Ô∏è\nSince I was very young I played soccer in the club Gimnasia y Esgrima de La Plata, after many years when I was in the 7th division I had to make the decision if soccer was going to be my professional destiny or a sport, the decision was to be a sport, this chapter deserves a separate publication.\nWhen I finished high school in La Plata I studied and graduated from the conservatory Gilardo Gilardi and at the same time I also graduated from a tertiary jazz school (EMU).\nAfter several albums, recitals and rehearsals I decided to turn my life around and started studying at the UTN (FRLP) the career of systems engineering.\nIn 2012, I started working in technology and thanks to it I made several friends that I still and fortunately keep, I will not name them because I‚Äôm afraid of forgetting more than one or one.\nAfter several years and now that I am writing it 38 and counting, it is a good time to write, I consider it a good channel to channel my feelings that adds to a good wine, music my family and friends.\nSo, here it goes‚Ä¶ I hope you enjoy it!"},"title":"Hi üëãüèº"},"/blog/posts/build-on-difference/":{"data":{"":"In this post, I‚Äôll try to share our experience building the IDP at Akua ‚Äî the challenges we faced, where we stand today, and what lies ahead for the platform.\nTogether with Ger, who at this point I can confidently call a friend since the early days of Akua, we built our internal developer platform. We come from different backgrounds and industries but share the same deep vision of what Akua needed.\nI left a link to his LinkedIn, but to make a very short (and somewhat unfair) summary, Ger comes from a background that spans from the lowest levels of infrastructure all the way to building the team and platform at Sate (for friends), or Satellogic if we‚Äôre being formal.\nAs for me, I started my career as a product developer, and already during my time at Viacom (Telef√©) I developed a strong curiosity for infrastructure. At Pomelo I had the opportunity to work full-time helping to build the IDP, and at Akua, together with ‚Äúmy buddy,‚Äù we designed and executed it from scratch ‚Äî something I‚Äôm incredibly proud of.","building-the-foundations#Building the Foundations":"We partnered with Binbash to execute our PCI-compliant network design while Ger and I evaluated which tool would best fit our infrastructure management needs for the IDP. To be fully transparent, we analyzed three options. We discarded one right away, and with the remaining two, we ran quick proof-of-concepts to decide which one to keep.\nThe three tools we evaluated were:\nTerraform CDK ‚Äî we ruled this one out from day zero. Why? Terraform was heading in a non‚Äìopen-source direction, and at that time the Terraform CDK project lacked solid documentation ‚Äî something that both Ger and I can‚Äôt stand due to how we work.\nCrossplane ‚Äî it‚Äôs tempting to use, but the moment you need to add any logic to a product your platform provides, you‚Äôre in trouble. And let‚Äôs be honest ‚Äî YAML is far too fragile to entrust it with managing your platform‚Äôs products.\nPulumi ‚Äî as you might‚Äôve guessed, this was our choice. It gave us the flexibility to implement any logic we wanted, abstracted away state management complexity, had excellent documentation, and a strong community adoption. That last point was key ‚Äî it meant that if we grew the team in the future, new members wouldn‚Äôt face something completely unfamiliar. Finally, and importantly, Pulumi allows the implementation of dynamic resources ‚Äî meaning if a resource isn‚Äôt natively supported, you can implement the Pulumi interface and manage it yourself. In our case (if I remember correctly), we did this twice: once for Typesense and once for New Relic deployment markers.\nTo clarify ‚Äî neither Ger nor I had used Pulumi in production before. We knew and had tested it, but hadn‚Äôt used it at scale yet.\nOnce we chose our backend tool, we moved to the next phase: defining the presentation layer of our IDP.\nAs the title of this post suggests, this was where we debated from our differences. Ger, coming from highly technical teams with a need for total control, proposed that we expose the platform‚Äôs abstractions and have developers use them directly in their projects ‚Äî meaning they‚Äôd need to know how to handle Pulumi. My perspective was that Akua‚Äôs developers wouldn‚Äôt feel comfortable dealing with platform tooling ‚Äî instead, we should provide a presentation layer (a portal) from which they could design their project‚Äôs architecture, deploy it, and manage it.\nJust like with our backend choice, two major options appeared here: Backstage and Port.io. There are others, of course, but we always aim for tools with strong industry adoption ‚Äî so that anyone joining later doesn‚Äôt find something obscure.\nIn this case, we chose Port.io. Why? Backstage would‚Äôve forced us to build components on top of it ‚Äî and at that moment, we didn‚Äôt want to touch any frontend work. It‚Äôs not our strength, and doing so would have meant significant time diversion we couldn‚Äôt afford.\nPort.io wasn‚Äôt a fallback ‚Äî quite the opposite. I like to describe Port as the Notion of platforms. Its Blueprints system lets you design your platform around your needs, not the other way around. The UI is elegant, supports SSO, offers RBAC (still improving), and includes features like Scorecards, Self-Service Actions, Automations, and dashboards that can be created in just a few clicks ‚Äî making it the perfect choice for our platform‚Äôs presentation layer.","conclusions#Conclusions":"There‚Äôs still a lot more to cover ‚Äî how we implemented scorecards, day-2 actions, or single-tenant deployments ‚Äî but I‚Äôll save that for another post.\nTo wrap up, I want to reflect on a few points.\nAs I mentioned, we didn‚Äôt always agree on everything. But we managed to build an internal developer platform from our differences, always respecting each other‚Äôs opinions. We created a workflow that led us to build solid, future-proof solutions, and to be honest ‚Äî were all the ideas Ger‚Äôs? Were they mine? Not at all. As I said, we built a process where we designed, evaluated multiple alternatives and cases, and then implemented. At the end of the day, it‚Äôs truly a team effort ‚Äî what matters most is our platform and the people behind it.\nDear Ger ‚Äî thank you deeply for your generosity and openness in teaching me so much. I hope I‚Äôve left my mark on you too. It‚Äôs an honor and a real pleasure to have built this platform together ‚Äî and, most importantly, the friendship we‚Äôve built along the way. That‚Äôs for life.\nUntil next time üëãüèΩ","security--auth-and-authz#Security ‚Äî Auth and Authz?":"From day zero, we‚Äôve used IRSA (IAM Roles for Service Accounts) with tag-based permissions (I‚Äôll mention our tagging strategy shortly). If a given AWS resource doesn‚Äôt support tags (like S3), our infra-lib can self-discover the resource and assign the required permissions automatically.\nSpeaking of tags ‚Äî in a previous post someone criticized that ‚Äúa platform without FinOps can‚Äôt be called a real platform.‚Äù Well, since day one, every single infrastructure resource we create includes centralized tagging managed by our infra-lib. This will allow us to easily implement billing for our IDP in the future. As a side note: our infrastructure costs are much lower than we predicted, thanks to correct per-environment configuration in our infra-lib.\nContinuing on the security topic ‚Äî access to resources like databases is managed via security group rules between apps and DBs. Likewise, traffic between applications is blocked at the network level, even within the same namespace. This allows us to establish service-to-service access policies managed by Kong, which can be configured easily from our Port.io portal.","technology-stack#Technology Stack":"We developed a custom Helm chart for our applications. Over time, it evolved ‚Äî now it supports deploying either a microservice or a monorepo, meaning a single project can deploy multiple Kubernetes services without needing separate GitLab projects.\nOur Helm chart (as designed from day zero) is unique, follows semver, and product developers can choose any version they need depending on the functionality required.\nAgain, our philosophy here was to centralize tooling and evolution, avoiding cognitive load on our internal customers. So, the Helm chart is shared ‚Äî each project defines its own values.yaml, which is used at deployment time for the specific application(s).\nFor our CI/CD and application lifecycle management, we use GitLab. Ger‚Äôs deep experience here was invaluable ‚Äî we run private runners that synchronize application resources without needing AWS credentials distributed in GitLab (which would be a major security risk).\nSpeaking of GitLab, we also know from experience how challenging it is to propagate pipeline changes across projects. That‚Äôs why we use centralized pipelines, where projects simply include them ‚Äî they don‚Äôt need to worry about anything else. When something new is needed, it‚Äôs added to the central pipeline and immediately available to everyone. As always, versioned with semver to ensure safe evolution.\nAs I might have mentioned, our application workloads run on Kubernetes ‚Äî specifically EKS on Fargate. This was Ger‚Äôs proposal, and thankfully, we were able to convince everyone necessary to go down that path.\nThe advantages? Minimal Kubernetes operations, trivial upgrades, PCI compliance, and more.\nOf course, there are tradeoffs ‚Äî you can‚Äôt deploy DaemonSets, for example. But that doesn‚Äôt worry us too much because (returning to our mantra of simplicity), our EKS clusters have very few installed components:\nKong Metrics Server External DNS And yes ‚Äî there must be a very strong justification and benefit to install anything else.","the-first-days-and-first-steps#The First Days and First Steps":"We started with a blank sheet ‚Äî literally. There was absolutely nothing at Akua. Just an empty AWS account, but that was the least of our worries. Here‚Äôs a short list of our ‚Äúmantras,‚Äù which we still uphold and make sure remain true to this day:\nThe operation and maintenance of our platform must tend to zero. Whatever we develop must be testable locally ‚Äî and it shouldn‚Äôt be a pain to do so. If it works in development, it must work in production (or any other environment). Everything should be self-discoverable ‚Äî no hardcoding. Simplicity above all. Break the inertia of past experiences.","wait--what-about-secrets#Wait ‚Äî what about secrets?":"We developed a workflow using Parameter Store, which allows us to deploy and make secrets available to applications flexibly. If a secret isn‚Äôt managed by our infra-lib, the security team can add it to Parameter Store and it automatically becomes available to the apps."},"title":"Building from Differences"},"/blog/posts/career/":{"data":{"":"Anniversaries, be it a birthday, the end of the year or whatever, generally lead one to reflect.\nThis case would not be the exception, what is the anniversary, less than a month ago i completed one year in my current company (Pomelo), August 29, 2022 the promotions in the company were made public, I am part of the people who were promoted, I wanted to write some conclusions that I have drawn from these 365 days, two teams and a promotion.","beginnings#Beginnings":"My first approach to Pomelo was through Maru, we already knew each other, Maru was the one who did the hiring process when I joined MercadoLibre, then we were close to meet again in Naranja X, but the impasse served well to meet again professionally in Pomelo, I tell this precisely because of the term of trust.\nDo you think Maru would have trusted me not just once, but twice, I don‚Äôt think so, she is an absolutely admirable person in building technology teams, and that trust we have in each other makes her trust me again and again and call me back:\nMaru trusts me again and again and calls me back. On my side, I take that trust and build (or try to build) the best possible work context.","meeting-and-mismatching#Meeting and mismatching":"In the more than 365 days that I have been in Pomelo I worked almost half of the time in the Processor team, I met again with familiar faces and met some new ones, I will name them, first the anecdote.\nI was at the Processor for a little more than 5 months, we reached the milestone (madness) of having the processor up and running and then in mid-January 2022 I moved to the infra team. Here two pillars were played, trust and respect, why I say this, the guys knew where I wanted to direct my career and supported me 100%.\nHowever, 7 months later they showed me that they are people in which I will trust infinitely beyond Pomelo, these people are: Brizi, Dami and Nico, I will not tell the details they know what I mean, but I wanted to thank them publicly ‚ù§Ô∏è.","pillars#Pillars":"Those who know me whether in my work or in my personal life, hear me talk about three principles with which I try to mobilize myself and i search around me:\nTrust Humility Respect","trust#Trust":"I will try to explain what this pillar means to me.\nIn one of my first publications where I talk about mistakes and learnings, I talk about the fact that moving up should not be the only or main goal, there are much, much more important things and confidence is one of them.\nOk luispi, but what does that have to do with trust?, here we go.","with-you-to-the-war-with-a-toothpick#With you, to the war with a toothpick":"I talked about my first stage in Pomelo, let‚Äôs move on to the second, ‚ÄúMigration to the Infrastructure team‚Äù.\nFrom January to this part and from minute zero Juanjo and Gus and knowing me, I would say nothing, gave me the space to create a space for debates and cross-examination. They listened to my proposals, made them their own, and together we carried them forward, they made parts of theirs I took them as my own and so surrounded by a pile of people with a human quality of another planet we did and we are doing incredible things.\nWhat is the point of all this, that getting promoted is not the main or only goal, building trust around you is countless times more important and is, at least in the values I pursue and seek, infinitely superior to any position we hold or obtain. any position we hold or obtain.\nFor now I do not want to bore or extend, soon I will write about the other two pillars that I feel indispensable to walk through life, humility and respect."},"title":"Anniversaries"},"/blog/posts/graceful-shutdown/":{"data":{"":"In this post, I want to share a topic that often gets overlooked when developing Go applications: the graceful shutdown.\nIt might seem like a minor detail, but it‚Äôs not. If you work with HTTP servers, workers, queue consumers, or any concurrent process using goroutines, handling shutdown properly can prevent data loss, hanging connections, or erratic behavior.","anti-pattern#Anti-pattern":"Let‚Äôs look at this code:\npackage main import ( \"log\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"hello world\")) }) log.Println(\"server listening on :8080\") http.ListenAndServe(\":8080\", nil) } At first glance, it works. But if you run this binary and stop it with ctrl + c, the process dies abruptly.\nAny ongoing requests are cut off, open resources (connections, files, etc.) aren‚Äôt released, and there‚Äôs no opportunity to perform cleanup.","conclusions#Conclusions":"Graceful shutdown isn‚Äôt optional; it‚Äôs a fundamental part of building reliable production services.\nUsing contexts properly and respecting shutdown timeouts is a simple way to avoid hard-to-reproduce bugs and improve the stability of your Go apps.\nTo avoid boring you for now, let‚Äôs pause here.\nSee you soon! üëãüèΩ","edge-cases#Edge cases":"Blocked goroutines: if a goroutine is waiting on a channel that no one closes, shutdown won‚Äôt complete.\nSolution: ensure channels are closed properly in the right sequence. Short timeout: if context.WithTimeout is too short, it might abort valid tasks. Tune it based on your real workload. Using defer inside goroutines: remember that defer statements execute when the goroutine returns. If it never returns, they never run.","mind-your-goroutines#Mind your goroutines":"Here‚Äôs where things get tricky.\nIf you have background goroutines ‚Äîlike workers consuming from a queue or processing tasks‚Äî, you need to ensure they also respect the cancellation context.\nA common mistake is to spawn goroutines that never stop:\ngo func() { for { processJob() // never stops } }() The result: when your server shuts down, this goroutine keeps running‚Ä¶ or dies abruptly, leaving half-done work.\nThe correct approach:\nctx, cancel := context.WithCancel(context.Background()) defer cancel() go func(ctx context.Context) { for { select { case \u003c-ctx.Done(): log.Println(\"worker stopped gracefully\") return default: processJob() } } }(ctx) When the process receives a signal, cancel() is called, and all goroutines listening to that context exit gracefully.","preamble#Preamble":"The concept of graceful shutdown is simple: give your application time to finish what it‚Äôs doing before dying.\nThe problem is that in Go, we often underestimate its complexity. For example, we start an HTTP server and a few background goroutines, assuming that ctrl + c or a SIGTERM will magically stop everything cleanly.\nSpoiler: it won‚Äôt. üòÖ","proposal--learning#Proposal / Learning":"The solution is to implement a graceful shutdown using Go‚Äôs context and os/signal packages.\npackage main import ( \"context\" \"log\" \"net/http\" \"os\" \"os/signal\" \"syscall\" \"time\" ) func main() { server := \u0026http.Server{Addr: \":8080\"} // Channel to listen for system signals stop := make(chan os.Signal, 1) signal.Notify(stop, os.Interrupt, syscall.SIGTERM) go func() { log.Println(\"server listening on :8080\") if err := server.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed { log.Fatalf(\"listen: %s \", err) } }() \u003c-stop // block until a signal is received log.Println(\"shutdown signal received\") ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := server.Shutdown(ctx); err != nil { log.Fatalf(\"server forced to shutdown: %v\", err) } log.Println(\"server stopped gracefully\") } Now, when the process receives a signal (SIGTERM or SIGINT), it has time (5 seconds in this case) to finish ongoing requests before closing.","sources-and-recommended-readings#Sources and recommended readings":"Go Blog: Go Concurrency Patterns ‚Äî Context Official Go context package docs Official Go os/signal package docs Official Go net/http package docs ‚Äî Server.Shutdown Go Wiki: Signal Handling"},"title":"Graceful shutdown and goroutines"},"/blog/posts/hexagonal-architecture/":{"data":{"":"This is the first of a series where we will review different development patterns that I made mistakes and what is currently and what I understand so far, the best way to apply it.\nAs the title of the publication says, today I want to talk about hexagonal architecture.","lets-start#Let\u0026rsquo;s start":"Anti pattern\nAnnotation in our domain model e.g json.\nOne mistake I used to make is having json or gorm annotations or any annotations in the domain model.\nLet‚Äôs imagine that we have our /user/user.go model with the following content\ntype User struct { FirstName string `json:\"first_name\"` LastName string `json:\"last_name\"` } Before reading on, I invite you to take a few seconds/minutes to reflect on why this is an anti-pattern.","preface#Preface":"I have been noticing a lot of ‚Äúhype‚Äù around the hexagonal architecture and I would like to be clear about it, I am not against it, on the contrary, I think it is an excellent pattern.\nBut I think we fall into the mistake of applying the publishing recipes of ‚Äúmedium‚Äù and not only end up with all the layers of our system coupled but with package names like ‚Äúadapters‚Äù or ‚Äúports‚Äù and if there is something beautiful in the language of Go(lang) is the intentionality in the name of a package.\nüóíÔ∏è A good package name means that nothing else is needed to express the intentionality of the package. I share and recommend you to read the following official publication, personal opinion, applies to any language.","proposallearning#Proposal/learning":"Our model should be presentation and data layer agnostic, therefore, it should not contain any annotations. We should map the data to our model and vice versa, here is an example of what a rest HTTP controller would look like e.g.:\nuser/rest_controller.go\ntype UserDTO struct { FirstName string `json:\"first_name\"` LastName string `json:\"last_name\"` } func (ctrl *controller) toUserModel(userDTO *UserDTO) *User { return \u0026User{ FirstName:\tuserDTO.FirstName, LastName:\tuserDTO.LastName, } } func (ctrl *controller) toUserDTO(user *User) *UserDTO { return \u0026UserDTO{ FirstName:\tuser.FirstName, LastName:\tuser.LastName, } } As we can see in the previous example the private methods in the rest controller map from the data transfer object to the domain model when we call the service and vice versa.\nFinally, we remove the annotations in our user/user.go model.\ntype User struct { FirstName string LastName string } All good luispi, but what gain did we get?\nNow our /user/user.go not having any annotation no matter how we present or how we obtain/persist the data, we will never touch the core/domain of our application, now we could affirm that our core/domain is agnostic to the presentation or data layers gaining flexibility, testing, etc.\nSo as not to bore you and for the moment let‚Äôs take a break.\nSoon we will continue with small publications where we will try to rethink other anti-patterns.\nHave a good time! üëãüèΩ"},"title":"Hexagonal Architecture"},"/blog/posts/interfaces-correct-use/":{"data":{"":"In the following post I want to share a common mistake I used to make in my Golang projects. And as the title and description says, today we are going to talk about interfaces.\nYou will probably come across some references to another post I did on hexagonal architecture, I think part of the misuse of interfaces is the consequence of following ‚Äúmedium‚Äù publications to the letter and not taking the time to understand the underlying concept.","conclusions#Conclusions":"With these subtle but powerful changes our producer now has enormous flexibility.\nFinally, I want to thank my friend and mentor morenojp who shared with me this anti pattern and made me rethink and improve, once again, in this software development.\nSo as not to bore you and for the moment let‚Äôs take a break.\nSoon we will continue with small publications where we will try to rethink other anti-patterns.\nHave a good time! üëãüèΩ\nSources:\nwiki oficial de Golang go interfaces misuse","preface#Preface":"If there is something that we have to admit in the Golang ecosystem, it is the kink that we sometimes give ourselves with some/several issues. To give an example on how to name variables (which is a topic I want to talk about soon) and in today‚Äôs case with interfaces.\nGolang has its particularities, but it is based on many patterns already known in the industry. In the Go ecosystem sometimes we complex some patterns and fall into ‚Äúanti patterns‚Äù, in the next post we will review a fake project and refactor it to make good use of the interfaces.\nTo simplify the publication a bit, we will narrow down the use case and reduce it to the service and repository layer.\nLet‚Äôs imagine that in the repository layer we find the following:\npackage repository import ( // pkg imports ) type Repository interface { Save(ctx context.Context, model *beer.Beer) (*beer.Beer, error) } type repository struct { // repository client and configs go here } func NewRepository() Repository { return \u0026repository{} } func (repo *repository) Save(ctx context.Context, model *beer.Beer) (*beer.Beer, error) { // previous logic here return repo.toModel(beerEntity), nil } And in the service layer the following:\npackage service import ( // pkg imports ) type Service interface { Create(ctx context.Context, model *Beer) (*Beer, error) } type service struct { repo Repository } func NewService(repo Repository) Service { return \u0026service{repo: repo} } func (svc *service) Create(ctx context.Context, model *Beer) (*Beer, error) { beer, err := svc.repo.Save(ctx, model) if err != nil { return nil, err } return beer, err } The above scenario is the one I commonly encounter in Go projects and I want to tell you that I also made the same mistake.\nAll good luispi, but what‚Äôs the mistake?\nGo interfaces generally belong to the package that uses values of the interface type, not to the package that implements those values. ü´†","proposallearning#Proposal/learning":"The implementation package must return concrete types (usually pointer or struct): that way, new methods can be added to implementations without requiring extensive refactoring.\nWith this in mind, let‚Äôs get to work\nFirst, let‚Äôs attack the repository layer, as the previous note says, we are going to return a structure and not the interface.\npackage repository import ( // pkg imports ) type Repository struct{ // repository client and configs go here } func NewRepository() Repository { return Repository{} } func (repo *Repository) Save(ctx context.Context, model *beer.Beer) (*beer.Beer, error) { // previous logic here return repo.toModel(beerEntity), nil } Let‚Äôs review the change.\nFirst we remove the interface and now the NewRepository() function returns the Repository structure, and second we add to Repository the save method.\nAll well and good luispi, but what do we gain with this change?\nSince we do not have to comply with any interface contract we are not tied to having to implement all the methods that the interface has.\nWe have to think of this package as a producer (producer) and always keep in mind that producers, again, return concrete types (usually a pointer or a structure).\nNow it is the turn to edit the consumer layer, in this case the service.\npackage service import ( // pkg imports ) type Repository interface { Save(ctx context.Context, beer *Beer) (*Beer, error) } type Service struct { repo Repository } func NewService(repo Repository) Service { return Service{repo: repo} } func (svc *Service) Create(ctx context.Context, beer *Beer) (*Beer, error) { createBeer, err := svc.repo.Save(ctx, beer) if err != nil { return nil, err } return createBeer, err } Let‚Äôs review the change.\nWe make several changes, first we declare the Repository interface and in the Service structure we inject the interface so that it can be consumed in the service methods.\nAs with the repository layer our NewService() now returns a structure and not an interface.\nFinally, we add the Create method to our Service."},"title":"Interfaces and \"copy paste\""},"/blog/posts/lightweight-container-image/":{"data":{"":"In the following post I am going to share with you some tips and best practices to develop our container images, as an example we are going to create an image for an app in Golang, but the following tips apply to any language, let‚Äôs go!","lets-go-with-the-first-proposal#Let\u0026rsquo;s go with the first proposal.":"It is always a good practice to use -alpine images, by convention in the container universe when we make available an -alpine image we are indicating to the client that it is an image reduced in size and the one we should use in our Dockerfile, among other things.\nWell, let‚Äôs make a small change in our Dockerfile and build our image again\nDockerfile 1 2 3 4 5 6 7 8 9 FROM golang:1.18-alpine3.16 WORKDIR /build COPY go.mod go.sum ./ RUN go mod download \u0026\u0026 go mod verify COPY . ./ RUN go build -o ./myapp ./path/to/main ENTRYPOINT [\"/myapp\"] If we pay attention the change was subtle, but effective, we went from FROM golang:1.18 to FROM golang:1.18-alpine3.16.\nLet‚Äôs build our docker build -t myapp:0.0.2 . image again.\nIf we list the images again we will find that now the myapp:0.0.2 image weighs approximately 331 MB.\nWe reduce, if the accounts do not fail, 637 MB.\nIt‚Äôs an excellent approach, but let‚Äôs rethink: do you have to have an image with all of Golang inside the container weighing about 331 MB to make available a binary that weighs a few megabytes?\nThe answer is clearly, no.","lets-start#Let\u0026rsquo;s start":"Let us imagine that we have the following Dockerfile to create our container image e.g:\nDockerfile 1 2 3 4 5 6 7 8 9 10 FROM golang:1.18 WORKDIR /build COPY go.mod go.sum ./ RUN go mod download \u0026\u0026 go mod verify COPY . ./ RUN go build -o ./myapp ./path/to/main ENTRYPOINT [\"/myapp\"] Let‚Äôs build our docker build -t myapp:0.0.1 . image.\nIf we list the images that we have in our host we will be able to observe that the weight is approximately 968 MB.\nWhat? 968 MB just to make available a binary that weighs a few megabytes?\nNOTE\nIn all my publications you will find concepts, the idea is that we learn and not copy and paste.\nTo give an example RUN go build -o ./myapp ./path/to/main where ./path/to/main should be the main of your Golang app","preface#Preface":"To make our container images as small as possible in terms of weight (megabytes, gigabytes, etc.) is not a matter of taste, it helps us in many ways, here are some of them:\nReduce storage costs in the registry we use to manage our images. When we have to obtain the image to start the container it is clear that the lighter it is the faster the initialization of the container will be, and with this we win in two points. Costs, and by costs we mean the use of the networking we use to obtain the image and then initialize the container. Speed in auto scaling, it is clear that to obtain a 20 MB image versus a 900 MB image the first one, of course, is going to initialize with a higher speed. To give a few examples.","proposallearning#Proposal/learning":"","second-proposal#Second proposal":"The container technology has an excellent feature, that for our case, will help us to build a very light container image, in case you didn‚Äôt know, I‚Äôm talking about Multistage, I share with you the official documentation to deepen about this feature.\nWhat does Multistage consist of? It is about building images in stages so that we can share data between each one of them, and we will obtain a final image of a very small size.\nThe first thing we are going to do is to have a first stage of build, where we are going to build the binary, and a second stage where we are going to make it available for use.\nLet‚Äôs get to work, let‚Äôs open and make the following modifications to our Dockerfile.\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # First layer use to build a Golang binary FROM golang:1.18-alpine3.16 AS builder WORKDIR /build COPY go.mod go.sum ./ RUN go mod download \u0026\u0026 go mod verify COPY . ./ RUN go build -o ./myapp ./path/to/main # Final layer expose app to minimal docker image FROM alpine:3.16.0 COPY --from=builder /build/myapp /myapp ENTRYPOINT [\"/myapp\"] As we can see, the first modification consists of tagging the first stage as a build. Then in the second and final stage with the following line COPY --from=builder /build/myapp /myapp we copy the binary from the stage that we have selected as builder and we make it available in an alpine image.\nIf we list now our images we can see that it weighs approximately 9 MB, yes yes, I wrote correctly 9 megabytes üòé.\nWe could do one last optimization or best practice, but I think it‚Äôs worth leaving it for another post.\nSo as not to bore you and for the moment let‚Äôs take a break.\nHave a good time! üëãüèΩ"},"title":"Size matters"},"/blog/posts/mistakes-and-learnings/":{"data":{"":"In our society in general and in technology in particular, we have the strange habit of publishing success stories (opinion of the writer, wrongly called successful case) and few, if any, occasions where mistakes are shared (opinion of the writer, wrongly called failure).\nAs you might be suspecting below I am going to share some mistakes, which I learned from them and even more how much I grew and understood about the industry by sharing them.","lesson-1#Lesson 1:":"Write a step-by-step, share it, validate and re-validate it before executing anything\nMy first productive development was in 2012 (?) approximately, it was a website for a company that needed to publish its product catalog, I list technologies to have some context:\ndigitalocean, just released, in addition to the droplets, I don‚Äôt know how many more products I had in my portfolio angular js (angular 1) I think nodeJS for the backend This development was done with one of the people I respect and admire the most, Victor De Grandis -Vitor-.\nMy experience configuring servers was null, I had the delirious idea of telling Vitor:\n_my friend, can I set up the server, and then you try to break it?\nThen I started to make several configurations, among them, I started to play with the ports, until I proudly told him:\nVitor, try to see what happens.\nAfter a while I receive a message,\nmy friend, could it be that you closed port 22?\nBeautiful, I left the house, locked the door and threw the key into a pond.\nAs the digitalocean people are prepared for people like this writer, they have a feature that if accessed from the web (I have no idea how to solve it from behind) you can run a terminal in the browser and access the droplet.\nI reconfigured the ports, and the rest is history.","lesson-2#Lesson 2:":"Document knowledge, no matter how trivial we consider it, and invite others to do the same. In addition to avoiding several ‚Äúknowledge transfer‚Äù meetings, we will save several arrobas, remind you anything?\n@fulanito may be able to help you\nIn another team that I had the fortune to work in, small in number of people but from which I learned a lot, a lot from each one of them, unfortunately the adventure was very short in time.\nWhat I did find very strange as one by one people were leaving the team is that we were losing a lot of knowledge to the point of stopping engines (literally) and going weeks without delivering features to production because we had to sit down and figure out how to leave that local line of code and expose it to the internet.\nWhat mistakes did I detect and what did I learn?\nDocument knowledge, however trivial we may consider it, document it.\nOr perhaps, and I invite you to think for a few seconds, did you find yourself one or more times holding ‚Äúknowledge transfer‚Äù meetings because someone was leaving the team?\nOr was it you who was leaving him?","lesson-3#Lesson 3:":"Job promotions should not be the only or the main objective, but the guide should be to share and acquire knowledge.\nLesson 3 bis:\nThinking and testing before implementing anything is a fundamental guide to understanding the behavior of our systems or the domain knowledge we have so far. Always, and let‚Äôs repeat, always, it is a gain in time.\nFinally, and in another place I worked, I met one of the best problem solvers, Juan Moreno, what is better for me, simple solutions to complex problems.\nAfter this small parenthesis and dedication to the one who today is one of my sources of consultation, reference and mentor, I am going to try to explain teaching number 3 and 3 bis.\nIn pre-pandemic times, I used to - a habit I still have from afar - start early, having a couple of hours ‚Äúalone‚Äù allows me to focus and prioritize as much as possible the things I want to do and have committed to do.\nOne of those mornings and as a consequence of a functionality that we were developing in the team, the leader arrived a little angry (to be kind) and grabbed the first ones he found from the team, thus being able to vent his anger because he had ‚Äúlooked bad‚Äù to his superiors because of all the casuistry there was a case that broke.\nFamous Take a breath and after a few minutes I joined him alone, I asked him a question:\nhow many people are involved in this functionality?\nHe understood with the question that he should have waited and told the whole team responsible what had happened and not grabbed the first people who crossed his path.\nFinally, I told him that I understood but did not share his anger or how he had handled it.\nI want to clarify that my intention is not to judge, but to share that I learned from that mistake, again:\nJob promotions should not be the only or main goal, you run the risk of being under enormous pressure not to rise to the occasion. Sharing and acquiring knowledge should be the guide.\nThinking and testing before implementing anything is the fundamental guide to understanding the behavior of our systems or the domain knowledge we have so far. Always, and let‚Äôs repeat, always, it is a gain in time.\nTo conclude, I understand that I will continue to make mistakes and share the experience they left me with, that does not make me any worse or better than anyone else.\nI am convinced that it is a healthy way of going through life where the ‚Äúnormal‚Äù thing to do is to be ‚Äúsuccessful‚Äù or as I like to make parallels with soccer, ‚Äúplaying for the stands‚Äù, but at the end of the day it is only an appearance.\nThank you for reading! üëãüèΩ"},"title":"You learn from your mistakes, and if you share them, you grow."},"/blog/posts/technical-debt/":{"data":{"":"In the following post we will rethink a concept commonly encountered and endorsed in the technology industry, technical debt.","conclusions#Conclusions":"The technical debt is very exhausting for the teams and where today we think we are gaining time, I assure you that soon, very soon, we will lose agility and speed in the development.\nIt can be avoided in most cases, and we do not accept it as a matter of course.\nFinally, I hope that we will rethink together the ‚Äútechnical debt‚Äù and that we will be uncomfortable with it.\nThank you for reading! üëãüèΩ","different-yardstick#Different yardstick":"Let‚Äôs imagine that we need to paint the exterior of our house, and we decide to hire a professional to do the job.\nAfter consulting several quotes we decided to hire the services of ‚Äúx‚Äù to take care of the job.\nAfter a few days ‚Äúx‚Äù tells us:\n- The work is finished.\nWe stood at the front of the house and noticed that around the window frames it is unpainted in some cases.\nNoticing this, we ask ‚Äúx‚Äù why he did not paint around the window frames, to which he responds:\n- Sorry, it is a technical debt that I can attack in the future.\nHow would you feel if you encountered such a situation?\nWhy in the context of technology do we endorse having technical debt?","rethink#Rethink":"In technology, it is very common to encounter this scenario, and unfortunately we naturalize it and as I commented in a previous publication where I talk among other things about the importance of documenting and testing, it is something common that I always see as ‚Äútechnical debt‚Äù, rethinking the example of painting the house I think it exposes how we can have ‚Äúdifferent yardsticks‚Äù to measure the same situation in different contexts.\nWhy is technical debt harmful?\nIn test context: it is extremely difficult to make changes without exploding everything. In context of documentation: it is extremely difficult to add functionality because the lack of documentation causes us to waste an enormous amount of time trying to understand how things are built. In a team context: having technical debt means that the knowledge stays with the people and we should always, always encourage knowledge to be distributed."},"title":"Technical debt"},"/blog/posts/ticket-hell/":{"data":{"":"","1-extremely-strong-culture#1. Extremely Strong Culture":"Infrastructure and QA teams are rarely included in roadmap and sprint planning. If they are, it usually happens at the very end ‚Äî ‚Äúwhen everything‚Äôs already cooked.‚Äù\nSo, when the infrastructure lead receives two tickets and needs to prioritize based on execution capacity, the obvious question arises:\nWhich ticket should I prioritize?\nGood luck with that ‚Äî because for every internal team (your ‚Äúclients‚Äù), their request will always be the most important thing in the world.","2-meaningless-growth#2. Meaningless Growth":"Let‚Äôs assume the previous issue is somehow solved. You still can‚Äôt escape this one.\nWhy do we say the infrastructure team must inherently grow?\nIf you fall into the pattern of having infrastructure handle all ticket requests, then as the company launches more products and features, you‚Äôll need more infrastructure engineers to handle the growing ticket demand.\nCongratulations ‚Äî you‚Äôve created an execution bottleneck.\nThe ‚Äúsolution‚Äù? Hire more infrastructure people.","3-motivation#3. Motivation":"Let‚Äôs assume points 1 and 2 are solved.\nBut‚Ä¶ are infrastructure engineers human? (They are, last I checked.)\nIt‚Äôs hard to stay motivated in a role that should be driven by creativity but instead is reduced to just closing tickets ‚Äî with no room for questioning or discussion.\nWhat happens next? High turnover in the infrastructure team and growing frustration among product or tech teams. And if those teams happen to be toxic, they might even start labeling infrastructure as slow or a blocker.\nAnd what can I say about that? Keep calm ‚Äî and here‚Äôs a big hug.","4-innovation#4. Innovation":"Finally, even if the previous three points were somehow resolved, innovation is still impossible.\nWhy? Because a team that only closes tickets ‚Äî that only participates reactively in projects to ‚Äúbuild infrastructure‚Äù ‚Äî will never have the space to innovate.","back-to-the-beginning#Back to the Beginning":"If all you have is a hammer, everything looks like a nail.\nThroughout this post, I‚Äôve explained the reasoning and main consequences of running infrastructure management through tickets.\nAnd I want to be clear: Working with tickets in an infrastructure team isn‚Äôt necessarily wrong. What‚Äôs absolutely wrong is managing infrastructure and platform operations through tickets.\nSo what does the ‚Äúhammer‚Äù metaphor have to do with this?\nIt means that many companies misapplied agile methodologies to their infrastructure teams. It‚Äôs time to pause and reflect on why things aren‚Äôt working well in so many places.\nHow can we avoid it?\nWe must inject a product mindset into infrastructure teams ‚Äî offering high-quality abstractions and user experiences over the underlying platform chaos, enabling product engineering teams to become self-sufficient in managing their software and its infrastructure.\nI hope my experience helps you rethink how to build culture and work processes for infrastructure teams within your company.\nIf you have questions or want to discuss any of this, I‚Äôd love to connect.\nSee you soon! üëãüèΩ","position#Position":"What‚Äôs the rationale behind such a strong stance?\nYou need an extremely strong culture from end to end to decide which ticket is more important than another. The infrastructure team must inherently grow as the tech and product teams expand to new features. It‚Äôs extremely difficult to keep the infrastructure team motivated. Innovation is impossible in a ticket-driven infrastructure model. There are many other reasons, but these are the most relevant ones ‚Äî let‚Äôs go through each.","problem#Problem":"If all you have is a hammer, everything looks like a nail.\nWhy do I start with this well-known phrase? First, I want to give a bit of background, and then I‚Äôll explain why I mention it.\nOver the years working with infrastructure teams, I‚Äôve had the great fortune of meeting some very insightful people who have always told me:\n‚ÄúNever let the infrastructure team operate solely through ticket requests.‚Äù"},"title":"The Ticket Hell"},"/blog/posts/we-should-allmanageteams/":{"data":{"":"For context in case you happened to come across this post, I‚Äôm going to speak from the technology industry, a deeply technical industry, and how leading teams, at least for a period of time, can help the team and yourself, both professionally and personally.","be-yourself-dont-manage-from-books#Be yourself, don\u0026rsquo;t manage from books":"Lastly, and this may also be a truism, but be yourself, you feel miles away when someone is talking to you from a prepared speech or tips from a leadership book.\nWe all are and continue to learn, you too.","do-i-have-personal-growth#Do I have personal growth?":"What would happen if we lead a team for at least one year?\nEven if it is not your chosen career path, leading and managing a team will provide you with very important tools, more important than any software design and architecture pattern.\nYou will learn to control your emotions, egos, frustrations, you will learn to negotiate, trust, advise, direct, ask for forgiveness and show your vulnerability, I could go on for a long time, but I think I can explain the level of information and tools it can give you.\nThe answer is yes, you are going to have personal and clearly professional growth.","exploit-the-virtues-and-reflect-on-the-improvements-both-are-equally-important#Exploit the virtues and reflect on the improvements, both are equally important":"I am going to state the obvious, we all have good and bad things.\nPlease when you have 1.1 talk about both things, exploit and emphasize the virtues and reflect on the improvements, if you only highlight the improvements ‚Äúor what is wrong‚Äù it is distressing for the person on the other side, he/she will feel that nothing is enough and nothing is right. On the contrary, if you only highlight the virtues and do not take the time to talk about the improvements, you will not drive change and improvement in the person‚Äôs life and career.\nAgain exploit the virtues and reflect on the improvements, both are equally important.","how-it-impacts-your-anxiety-and-that-of-the-team-to-manage-it#How it impacts your anxiety and that of the team to manage it":"I am from Argentina, a country with the highest number of psychologists per inhabitant, it is neither a good nor a bad thing, in fact I have been doing therapy for more than ten years to rethink myself and to rethink society.\nI bring this data because of the subject of anxiety and it deserves a separate chapter, as social networks have intensified this phenomenon.\nBut let‚Äôs get back to the point I want to develop, anxiety in team management.\nWe live in a society that is in a hurry to get where we develop applications that encourage sharing videos of seconds because we can not stand to see a video longer than 30 seconds, where sharing information has to be compressed into 140 characters and if it is a longer thread uf not even want to read it. In this context of anxiety, managing and developing teams is a huge challenge, almost titanic I would say.\nStolen from a lecture by Paenza full lecture\nThere should be no other year in a person‚Äôs life where one incorporates so much information, learns so much, changes so much in quality of life, as from 0 to 1 year of age.\nWhere am I going with this point?\nThe contributions that one makes to the team and the people are very, veeeery slow, and in most cases almost imperceptible and you will have to learn to manage your anxiety and support the team to manage theirs.\nAnother point that I find very curious is that ability (and this is also commented by the great Paenza in the talk I shared with you recently) to have zero patience to face a problem, we need answers for everything, and yes, that is partly due to the anxiety we handle. Let‚Äôs encourage the ability to solve problems, not everything in life has a solution and there are no answers for everything.\nLet‚Äôs encourage the ability to solve problems, not everything in life has a solution and there are no answers for everything.\nFinally, in many companies it is common to encounter performance review processes twice a year and I would like to leave a question in the air.\nDo we really believe that a person can make an evolution within six months?\nAnd here the fragment of Paenza‚Äôs talk makes more sense.\nThere should be no other year in a person‚Äôs life where one incorporates so much information, learns so much, changes so much in quality of life, as from 0 to 1 year of age.","the-obvious-not-everything-has-to-go-through-you-nor-do-you-have-to-be-aware-of-everything#The obvious, not everything has to go through you, nor do you have to be aware of everything":"How many times have you crossed paths with ‚Äútoxic leaders‚Äù, What is toxic for me?\nThose people who are funnel, everything has to go through them, all the time they need different decisions to go through them and almost 100% of the time their decisions predominate.\nWhat do I propose?\nLet‚Äôs promote and accompany the growth of the people in the team and the team, let‚Äôs share knowledge and be open to new ideas and proposals.\nPersonally speaking the day I went on vacation and everything followed its course, we even took out functionalities, we communicated them and when you come back you don‚Äôt have that feeling of FOMO it‚Äôs the glory.\nI leave the question in the context of when you come back or someone comes back from vacation and you hear it daily.\nUff I‚Äôve been going through all the emails and slack messages to catch up\nShouldn‚Äôt projects take their natural course without you being present?","the-quest-for-quick-feedback-to-feel-valuable#The quest for quick feedback to feel valuable":"One of the things I would have loved to have been advised on when I started managing and leading teams is to look for things that give me a quick return of dopamine.\nWhen you are leading and managing teams, change processes are very slow and you tend to think that you are useless and that you are not doing ‚Äúanything productive‚Äù, calm down.\nI don‚Äôt like tips and tricks, but something that helped me and that I would have liked to be advised is to look for what makes you happy, write a piece of code or refactor, document, anything, but something that makes you feel that you are contributing value to the team, again, already leading and making decisions bring a brutal value, the problem is that it is imperceptible and one is human and needs to feel useful and valuable."},"title":"We should all manage teams"},"/blog/posts/when-the-context-matter/":{"data":{"":"The following publication could be said to be a continuation of another one where we talk about how having lightweight images helps us in many aspects, if you still could not read it here I leave you the access.\nToday we are going to make a small, but important improvement, and we are going to find out why we are doing it.","preface#Preface":"As the last proposal in the post I shared earlier we stayed at this point:\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # First layer use to build a Golang binary FROM golang:1.18-alpine3.16 AS builder WORKDIR /build COPY go.mod go.sum ./ RUN go mod download \u0026\u0026 go mod verify COPY . ./ RUN GOOS=linux go build -o ./myapp ./path/to/main # Final layer expose app to minimal docker image FROM alpine:3.16.0 COPY --from=builder /build/myapp /myapp ENTRYPOINT [\"/myapp\"] If we have as a premise that container technology and its popularization with Docker is disruptive is largely because of the benefits of being able to build in different places and not find surprises when we start the application that is contained in the container, I invite you to think for a few seconds / minutes or as long as we need.\nCan an image enhancement be made for the Golang application?\nThe answer is yes, let‚Äôs get to work!","proposallearning#Proposal/learning":"Golang has one feature that is really powerful, and I‚Äôm not talking about goroutines, and that is the great virtue of being able to cross-compile.\nWhat is cross-compilation?, it is the feature of being able to compile from a host with a certain architecture and operating system (OS) the binary for another architecture or OS.\nSo to be a little more specific we can from a host with OS = linux and architecture = amd64, compile a binary for OS = windows, architecture = 386 üò≤.\nLet‚Äôs imagine now that where we run the containers for our applications the computation is linux as OS and with amd64 architecture.\nWith this in mind let‚Äôs make a small but important improvement to our Dockerfile.\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # First layer use to build a Golang binary FROM golang:1.18-alpine3.16 AS builder WORKDIR /build COPY go.mod go.sum ./ RUN go mod download \u0026\u0026 go mod verify ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 COPY . ./ RUN go build -o ./myapp ./path/to/main # Final layer expose app to minimal docker image FROM alpine:3.16.0 COPY --from=builder /build/myapp /myapp ENTRYPOINT [\"/myapp\"] Let‚Äôs first analyze the change and why we are making it.\nENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 CGO_ENABLED=0 we deactivate CGO\nGOARCH=amd64 we indicate the architecture\nGOOS=linux we indicate the SO\nAll good luispi, but what gain did we get?\nMaking sure to compile the application for the environment in which it will be executed will prevent several headaches or troubleshooting, and needless to say that we no longer care where we are going to do it (whatever our continuous integration channel), because by compiling, again, for the environment in which it will be executed, we can rest assured that we are shortening the margin of mishaps.\nSo as not to bore you and for the moment let‚Äôs take a break.\n¬°Have a good time! üëãüèΩ"},"title":"When context matters"}}